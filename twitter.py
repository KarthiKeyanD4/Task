# -*- coding: utf-8 -*-
"""Twitter.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VCS8WYD0p_ircSvBzc3TyvN_vuXF2PVF
"""

# Packages for commmon functions
import re
import string
import pandas as pd
import numpy as np

# Packages mostly used for vectorizing and lemmation
import spacy

# Packages for modelling
from sklearn.svm import LinearSVC
from sklearn.linear_model import LogisticRegression
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.metrics import classification_report

# Setting the english dictionary
nlp = spacy.load('en_core_web_sm')

# Function for preprossing where lemmatizing and stop words are removed
def preprocess(text):
    doc = nlp(text)
    token_list = []
    for token in doc:
        if token.is_stop == False and token.lemma_.isalpha() == True: # it will filter the symbols and stopwords from the dictionary
            token_list.append(token.lemma_)
    return token_list

def vectorize(text):
  vector = TfidfVectorizer(tokenizer = preprocess) # using the preprocessing function as tokenizer in Tfid-Vectorize
  X = vector.fit_transform(text).toarray()
  return X

"""Usually, The given situation is regarding degree of profanity. We could consider the output to be binary i.e) 0 - No Racist comments, 1 - Racist Comment.

Since the output is binary, We could go for Logistic Regression or SVM for modelling
"""

def model_1(X,y):
  model = LinearSVC(class_weight='balanced',C=0.01, penalty='l2', loss='squared_hinge',).fit(X, y)
  y_pred = model.predict(X)
  report = classification_report(y,y_pred)
  return [report, y_pred]

"""An Example with a data set. """

df = pd.read_csv("labeled_data.csv")

X = vectorize(df['tweet'])

y = df['class']

X = pd.DataFrame(X)

G = model_1(X,y)

print(f"The summary of the model is \n {G[0]}")

print(f"The first 5 prediction of y values is \n {G[1][:5]}")